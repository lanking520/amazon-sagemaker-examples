{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Running LLaMA model with SageMaker Inference endpoint\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "Please review [LLaMA License](https://github.com/facebookresearch/llama/blob/main/LICENSE) before running this example.\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105410e",
   "metadata": {},
   "source": [
    "Here, we would assume your LLaMA model is stored in a folder of a S3 bucket.\n",
    "\n",
    "**Note: We do expect the LLaMA model came from its original form. Please make sure you have the original checkpoints.**\n",
    "\n",
    "```\n",
    "aws s3 ls s3://bucket/some_llama_model\n",
    "- tokenizer.model\n",
    "- consolidated.00.pth\n",
    "- consolidated.01.pth\n",
    "- consolidated.02.pth\n",
    "- consolidated.03.pth\n",
    "```\n",
    "\n",
    "The above one showed a 4-checkpoint LLaMA model that we expect to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6210dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = \"s3://bucket/some_llama_model/\"\n",
    "print(f\"You can set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties: Defines the model server settings\n",
    "- model.py: A python file to define the core inference logic\n",
    "- requirements.txt: Any additional pip wheel need to install\n",
    "\n",
    "**Remember to change your S3 path to the one you have the model artifacts**\n",
    "\n",
    "**If your LLAMA model has 8 checkpoints, please change the tensor_parallel_degree below to 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=4\n",
    "option.s3url=s3://bucket/some_llama_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "from djl_python import Input, Output\n",
    "\n",
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # seed must be the same in all processes\n",
    "    torch.manual_seed(1)\n",
    "    return local_rank, world_size\n",
    "\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    local_rank: int,\n",
    "    world_size: int,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    ") -> LLaMA:\n",
    "    start_time = time.time()\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "    assert world_size == len(\n",
    "        checkpoints\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    model = Transformer(model_args)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    generator = LLaMA(model, tokenizer)\n",
    "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    return generator\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    if \"model_id\" in properties:\n",
    "        ckpt_dir = properties[\"model_id\"]\n",
    "        tokenizer_path = os.path.join(properties[\"model_id\"], \"tokenizer.model\")\n",
    "    else:\n",
    "        ckpt_dir = properties[\"ckpt_dir\"]\n",
    "        tokenizer_path = properties[\"tokenizer_path\"]\n",
    "    max_seq_len= 512\n",
    "    max_batch_size = 32\n",
    "    local_rank, world_size = setup_model_parallel()\n",
    "    if local_rank > 0:\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    return load(\n",
    "        ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size\n",
    "    )\n",
    "\n",
    "def infer(generator, inputs):\n",
    "    temperature = 0.8\n",
    "    top_p= 0.95\n",
    "    prompts = inputs[\"prompts\"]\n",
    "    results = generator.generate(\n",
    "        prompts, max_gen_len=256, temperature=temperature, top_p=top_p\n",
    "    )\n",
    "    return results\n",
    "\n",
    "generator = None\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global generator\n",
    "    if not generator:\n",
    "        generator = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()\n",
    "    result = infer(generator, data)\n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0142973",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "git clone https://github.com/facebookresearch/llama\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "mv model.py mymodel/\n",
    "mv llama/requirements.txt mymodel/\n",
    "mv llama/llama mymodel/llama\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.21.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names\n",
    "\n",
    "For different LLaMA model, here is the recommendation:\n",
    "\n",
    "|  Model | MP | Instance (min) | Instance (best)    |\n",
    "|--------|----|------------|------------------------|\n",
    "| 7B     | 1  | g5.2xlarge | p4d.24xlarge | \n",
    "| 13B    | 2  | g5.12xlarge| p4d.24xlarge | \n",
    "| 33B    | 4  | g5.12xlarge| p4d.24xlarge | \n",
    "| 65B    | 8  | g5.48xlarge| p4d.24xlarge | \n",
    "\n",
    "G5 is minimum requirement to host a LLaMA model. If you would like to run in long sequence (e.g 2048/4096). Please consider using a P4D instance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcef095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n3 -r1\n",
    "predictor.predict(\n",
    "    {\"prompts\": \"Large model inference is\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
